{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45538725-944b-4bc0-9ebc-071b93c6e602",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Gradient Boosting Regression is a popular machine learning algorithm used for regression tasks. It is an ensemble learning method that combines multiple weak regression models, typically decision trees, to create a strong predictive model. The algorithm iteratively builds an ensemble of weak models by minimizing a loss function through gradient descent.\n",
    "\n",
    "Here's an overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. Initialization: Initialize the ensemble with a simple model, such as a decision tree with a small depth or a constant value as the initial prediction.\n",
    "\n",
    "2. Iterative Training:\n",
    "   a. Compute Residuals: Calculate the difference between the actual target values and the predictions made by the current ensemble.\n",
    "   \n",
    "   b. Train a Weak Learner: Fit a weak regression model, such as a decision tree, to the residuals. The weak learner aims to capture the patterns or relationships in the residuals.\n",
    "   \n",
    "   c. Update Ensemble: Add the newly trained weak learner to the ensemble. The weak learner's prediction is multiplied by a learning rate, which controls the contribution of each weak learner to the ensemble.\n",
    "   \n",
    "   d. Update Predictions: Update the predictions of the ensemble by adding the predictions from the new weak learner, scaled by the learning rate.\n",
    "   \n",
    "   e. Repeat Steps a to d: Iterate the process by computing the residuals again using the updated predictions and training new weak learners on the residuals. The ensemble gradually improves by focusing on the errors made by the previous weak learners.\n",
    "   \n",
    "3. Final Prediction: The final prediction is obtained by combining the predictions from all the weak learners in the ensemble. In regression tasks, the predictions are typically averaged or weighted according to their respective learning rates.\n",
    "\n",
    "Gradient Boosting Regression aims to minimize a loss function by iteratively adding weak learners to the ensemble. The algorithm focuses on the residuals or errors made by the previous weak learners, allowing subsequent weak learners to learn from these mistakes and improve the ensemble's predictive power. Through this iterative process, Gradient Boosting Regression can effectively capture complex relationships in the data and provide accurate regression predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07e162-722b-4b5c-ba7c-862d722f4500",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Certainly! Here's an example of a simple gradient boosting algorithm implemented from scratch using Python and NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.estimators = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.estimators = []\n",
    "        self.base_prediction = np.mean(y)  # Initial prediction is the mean of target values\n",
    "        \n",
    "        # Initialize residuals with y - base_prediction\n",
    "        residuals = y - self.base_prediction\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            estimator = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            estimator.fit(X, residuals)\n",
    "            \n",
    "            self.estimators.append(estimator)\n",
    "            \n",
    "            # Update residuals based on the new estimator's predictions\n",
    "            residuals -= self.learning_rate * estimator.predict(X)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.base_prediction)\n",
    "        for estimator in self.estimators:\n",
    "            y_pred += self.learning_rate * estimator.predict(X)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Initialize and fit the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "```\n",
    "\n",
    "In this example, we create a simple dataset with one input feature (X) and one target variable (y). We then initialize and fit the `GradientBoostingRegressor` to the dataset. After fitting, we make predictions on the same dataset and evaluate the model's performance using mean squared error (MSE) and R-squared metrics. The implementation uses `DecisionTreeRegressor` from scikit-learn as the weak learner. Note that this is a simplified implementation, and in practice, gradient boosting libraries such as XGBoost or LightGBM provide more optimized and efficient implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656d58f-5816-475b-8071-003afb871f0c",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "To experiment with different hyperparameters and find the best combination, you can use either grid search or random search. Here's an example of how to perform hyperparameter tuning using random search in the context of gradient boosting regression:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_grid = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'learning_rate': uniform(0.01, 0.5),\n",
    "    'max_depth': randint(2, 10)\n",
    "}\n",
    "\n",
    "# Initialize the gradient boosting regressor\n",
    "regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(regressor, param_distributions=param_grid, n_iter=10, cv=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = random_search.best_estimator_\n",
    "y_pred = best_model.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (Best Model):\", mse)\n",
    "print(\"R-squared (Best Model):\", r2)\n",
    "```\n",
    "\n",
    "In this example, we use the Boston Housing dataset and define a parameter grid for the random search. The parameter grid specifies the ranges of values for the hyperparameters `n_estimators`, `learning_rate`, and `max_depth`. We then initialize a gradient boosting regressor and perform random search with `RandomizedSearchCV`. The `n_iter` parameter controls the number of random combinations of hyperparameters to try, and `cv` specifies the number of cross-validation folds. The scoring metric used is negative mean squared error (`neg_mean_squared_error`), as the goal is to minimize the error. Finally, we print the best hyperparameters and evaluate the performance of the best model using mean squared error and R-squared.\n",
    "\n",
    "Note that you can also use grid search (`GridSearchCV`) instead of random search if you want to exhaustively search all possible combinations of hyperparameters. Grid search is more computationally expensive but can be useful when the search space is relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d409a-e624-4911-8e58-c92f16b8affa",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "In the context of gradient boosting, a weak learner refers to a simple model or a base model that performs slightly better than random guessing on a given task. It is a model that has limited predictive power on its own but can still contribute to the overall performance when combined with other weak learners in an ensemble.\n",
    "\n",
    "In practice, decision trees are commonly used as weak learners in gradient boosting algorithms. However, other models such as linear regression models or shallow neural networks can also be used as weak learners depending on the problem and algorithm implementation.\n",
    "\n",
    "The weak learner is trained on the residuals or errors made by the previous weak learners in the ensemble. It aims to capture the patterns or relationships in the residuals and provide better predictions than the previous models. The key idea behind boosting is to sequentially add these weak learners to the ensemble and focus on the mistakes made by the previous models, gradually improving the ensemble's performance.\n",
    "\n",
    "Each weak learner in the ensemble contributes a small amount to the overall prediction, typically weighted by a learning rate, and the combined predictions of all weak learners create a strong learner that can capture complex relationships in the data. By iteratively adding and updating the weak learners, gradient boosting effectively reduces the overall bias and variance of the ensemble, leading to better predictive performance.\n",
    "\n",
    "The term \"weak learner\" should not be confused with a weak model in general. In the context of gradient boosting, a weak learner can still contribute significantly to the ensemble's performance when combined with other weak learners, resulting in a strong overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addb7d8-1f34-48d9-b638-aff0ec2715a1",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. Start with an initial prediction: The algorithm begins by making an initial prediction based on a simple model, such as the mean value of the target variable for regression problems or the most frequent class for classification problems. This initial prediction serves as a starting point.\n",
    "\n",
    "2. Iteratively improve the model: The algorithm iteratively builds a sequence of weak models, where each model tries to correct the mistakes made by the previous models. In each iteration, a new weak model is trained on the residuals or errors of the previous model.\n",
    "\n",
    "3. Focus on the errors: The subsequent weak models are designed to focus on the errors or residuals of the previous models. By sequentially learning from the mistakes, the algorithm gradually reduces the errors and improves the overall model performance.\n",
    "\n",
    "4. Weighted contribution: Each weak model's predictions are weighted based on their performance and combined to create the final prediction. The weights are determined by the learning rate, which controls the contribution of each weak model to the final ensemble. By adjusting the learning rate, the algorithm can control the impact of each weak model on the final prediction.\n",
    "\n",
    "5. Reduce the overall error: The algorithm aims to minimize a loss function, such as mean squared error for regression or log loss for classification. In each iteration, the weak model is trained to minimize the loss function with respect to the residuals or errors of the previous models.\n",
    "\n",
    "6. Prevent overfitting: To prevent overfitting, the algorithm typically includes regularization techniques, such as tree depth restrictions or shrinkage parameters, which control the complexity of the weak models and the overall ensemble.\n",
    "\n",
    "7. Final prediction: The final prediction is the sum of the initial prediction and the predictions from all the weak models, each weighted by their respective learning rate. This combination of weak models produces a strong model with improved performance.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm lies in its ability to leverage the strengths of weak models by iteratively learning and improving from the mistakes made by previous models. By combining the predictions of multiple weak models, the algorithm builds a powerful ensemble that can capture complex patterns and relationships in the data, resulting in highly accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273823c9-d775-41c7-91d6-b39e31c09817",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how it constructs the ensemble:\n",
    "\n",
    "1. Initialize the ensemble: The algorithm starts by initializing the ensemble with a simple model, usually a constant prediction or a naive prediction based on the mean value of the target variable.\n",
    "\n",
    "2. Calculate the initial residuals: The initial residuals are calculated as the differences between the actual target values and the predictions made by the initial model. These residuals represent the errors made by the initial model and serve as the target variable for subsequent weak learners.\n",
    "\n",
    "3. Train a weak learner: A weak learner, typically a decision tree, is trained on the residuals from the previous step. The goal of the weak learner is to capture the patterns or relationships in the residuals and provide a better prediction than the previous model. The weak learner is typically trained using gradient descent or a similar optimization algorithm to minimize the loss function with respect to the residuals.\n",
    "\n",
    "4. Update the ensemble: The predictions of the weak learner are added to the predictions made by the previous models in the ensemble. However, the contribution of the weak learner's prediction is controlled by a learning rate parameter. The learning rate determines the weight or importance given to the weak learner's prediction. A smaller learning rate means a more cautious update to the ensemble, while a larger learning rate allows for a more significant update.\n",
    "\n",
    "5. Update the residuals: The residuals are updated by subtracting the predictions made by the weak learner from the previous residuals. The updated residuals represent the remaining errors or unexplained variation in the target variable that the ensemble needs to learn.\n",
    "\n",
    "6. Iterate steps 3-5: Steps 3 to 5 are repeated iteratively for a predetermined number of iterations or until a convergence criterion is met. In each iteration, a new weak learner is trained on the updated residuals, the ensemble is updated with the new predictions, and the residuals are updated based on the new predictions.\n",
    "\n",
    "7. Final ensemble prediction: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. Each weak learner's prediction is weighted by the learning rate to control its contribution. The sum of the predictions represents the ensemble's final prediction.\n",
    "\n",
    "By sequentially training weak learners on the residuals and updating the ensemble, the Gradient Boosting algorithm progressively reduces the errors and improves the predictions. The ensemble of weak learners is designed to collectively capture the complex relationships and patterns in the data, resulting in a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b95269c-217d-40ac-9467-1a4a91f67f7a",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. Define the loss function: The first step is to define an appropriate loss function that measures the discrepancy between the predicted values and the actual values. The choice of the loss function depends on the problem at hand, such as mean squared error for regression or log loss for classification.\n",
    "\n",
    "2. Initialize the ensemble: The algorithm starts with an initial model that provides a baseline prediction. This initial prediction can be a simple estimate, such as the mean value of the target variable for regression problems or the most frequent class for classification problems.\n",
    "\n",
    "3. Compute the negative gradient of the loss function: The negative gradient of the loss function with respect to the predicted values is calculated. This gradient represents the direction in which the loss function decreases the most. It serves as a measure of the errors or residuals in the current model's predictions.\n",
    "\n",
    "4. Train a weak learner to fit the negative gradient: A weak learner, often a decision tree, is trained to fit the negative gradient calculated in the previous step. The weak learner is trained to approximate the mapping between the input features and the negative gradient values. The tree is typically shallow and is constrained by parameters such as maximum depth or minimum number of samples per leaf.\n",
    "\n",
    "5. Update the ensemble: The weak learner's predictions are combined with the predictions made by the previous models in the ensemble. Each weak learner's prediction is weighted by a learning rate, which determines its contribution to the ensemble. The learning rate controls the impact of each weak learner on the final prediction.\n",
    "\n",
    "6. Update the residuals: The residuals are updated by subtracting the predictions made by the weak learner from the previous residuals. The updated residuals represent the remaining errors or unexplained variation in the target variable that the ensemble needs to learn.\n",
    "\n",
    "7. Repeat steps 3-6: Steps 3 to 6 are repeated iteratively for a predetermined number of iterations or until a convergence criterion is met. In each iteration, a new weak learner is trained on the updated residuals, the ensemble is updated with the new predictions, and the residuals are updated based on the new predictions.\n",
    "\n",
    "8. Final ensemble prediction: The final prediction is obtained by summing the predictions made by all the weak learners in the ensemble. Each weak learner's prediction is weighted by the learning rate to control its contribution. The sum of the predictions represents the ensemble's final prediction.\n",
    "\n",
    "By iteratively fitting weak learners to the negative gradients and updating the ensemble based on their predictions, the Gradient Boosting algorithm progressively reduces the errors and improves the predictions. The algorithm's mathematical intuition lies in minimizing the loss function by iteratively approximating the negative gradients with a sequence of weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867dfbb5-393e-4225-bad4-c3d6104c119d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
